{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72db6212",
   "metadata": {},
   "source": [
    "# Q&A Model based on Custom Data using GooglePalm LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae384fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "api_key=\"AIzaSyDWISyWAdPJlm57KB3Vcp2nYqGA0rta-XU\"\n",
    "llm=GooglePalm(google_api_key=api_key,temperature=.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91747e",
   "metadata": {},
   "source": [
    "### Testing the GooglePalm LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "095b9bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear [Customer Service Representative name],\n",
      "\n",
      "I am writing to request a refund for the [product name] that I purchased on [date]. I am not satisfied with the product because [reason for dissatisfaction].\n",
      "\n",
      "I have attached a copy of my receipt and a picture of the product. I have also included a brief description of the problem that I am having with the product.\n",
      "\n",
      "I would appreciate it if you would process my refund as soon as possible. I would like to receive a refund in the form of a credit to my original payment method.\n",
      "\n",
      "Thank you for your time and attention to this matter.\n",
      "\n",
      "Sincerely,\n",
      "[Your name]\n"
     ]
    }
   ],
   "source": [
    "essay = llm(\"write email requesting refund for electronic item\")\n",
    "print(essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb14e6",
   "metadata": {},
   "source": [
    "### Now let's load data from Codebasics FAQ csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3354d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "loader=CSVLoader(file_path='codebasics_faqs.csv',source_column='prompt')\n",
    "data=loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7469e391",
   "metadata": {},
   "source": [
    "# Hugging Face Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84ba9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f49fb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:\\Users\\rohan/.cache\\torch\\sentence_transformers\\hkunlp_instructor-large\\ were not used when initializing T5EncoderModel: ['vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'text_model.embeddings.position_embedding.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'logit_scale', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'text_projection.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'visual_projection.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'text_model.final_layer_norm.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.pre_layrnorm.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'text_model.embeddings.position_ids', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.embeddings.class_embedding', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at C:\\Users\\rohan/.cache\\torch\\sentence_transformers\\hkunlp_instructor-large\\ and are newly initialized: ['encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.19.layer.0.layer_norm.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.22.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.22.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.23.layer.0.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.22.layer.0.SelfAttention.v.weight', 'encoder.block.12.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.17.layer.0.SelfAttention.v.weight', 'encoder.block.20.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.15.layer.0.SelfAttention.o.weight', 'encoder.block.15.layer.1.DenseReluDense.wo.weight', 'encoder.block.16.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.21.layer.1.DenseReluDense.wi.weight', 'encoder.block.13.layer.0.layer_norm.weight', 'encoder.block.20.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.19.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.13.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.12.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.12.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.14.layer.1.DenseReluDense.wo.weight', 'encoder.block.18.layer.0.SelfAttention.v.weight', 'encoder.block.22.layer.0.SelfAttention.q.weight', 'encoder.block.21.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.16.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.19.layer.0.SelfAttention.o.weight', 'encoder.block.14.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.13.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.20.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.12.layer.0.layer_norm.weight', 'encoder.block.13.layer.0.SelfAttention.q.weight', 'encoder.block.18.layer.1.layer_norm.weight', 'encoder.block.18.layer.1.DenseReluDense.wi.weight', 'encoder.block.21.layer.1.DenseReluDense.wo.weight', 'encoder.block.17.layer.1.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.23.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.15.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.22.layer.0.layer_norm.weight', 'encoder.block.16.layer.0.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.15.layer.0.layer_norm.weight', 'encoder.block.17.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.19.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.23.layer.0.SelfAttention.k.weight', 'encoder.block.22.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.14.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.21.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.15.layer.1.DenseReluDense.wi.weight', 'encoder.block.17.layer.0.layer_norm.weight', 'encoder.block.19.layer.1.layer_norm.weight', 'encoder.block.21.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.14.layer.0.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.18.layer.0.SelfAttention.q.weight', 'encoder.block.19.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.20.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.12.layer.1.DenseReluDense.wi.weight', 'encoder.block.16.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.16.layer.0.SelfAttention.o.weight', 'encoder.block.15.layer.0.SelfAttention.k.weight', 'encoder.block.16.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.13.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.23.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.13.layer.0.SelfAttention.v.weight', 'encoder.block.23.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.14.layer.1.layer_norm.weight', 'encoder.block.23.layer.1.layer_norm.weight', 'encoder.block.20.layer.1.layer_norm.weight', 'encoder.block.17.layer.1.DenseReluDense.wo.weight', 'encoder.block.19.layer.0.SelfAttention.v.weight', 'shared.weight', 'encoder.block.14.layer.0.SelfAttention.o.weight', 'encoder.block.18.layer.0.layer_norm.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.14.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.21.layer.0.SelfAttention.k.weight', 'encoder.block.18.layer.0.SelfAttention.o.weight', 'encoder.embed_tokens.weight', 'encoder.block.14.layer.0.SelfAttention.q.weight', 'encoder.block.22.layer.1.layer_norm.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.17.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.19.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.16.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.13.layer.1.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.21.layer.0.SelfAttention.o.weight', 'encoder.block.15.layer.0.SelfAttention.v.weight', 'encoder.block.20.layer.0.SelfAttention.k.weight', 'encoder.block.12.layer.0.SelfAttention.v.weight', 'encoder.block.18.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.23.layer.0.SelfAttention.o.weight', 'encoder.block.23.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.12.layer.0.SelfAttention.k.weight', 'encoder.block.17.layer.0.SelfAttention.q.weight', 'encoder.block.21.layer.0.SelfAttention.v.weight', 'encoder.final_layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.12.layer.1.DenseReluDense.wo.weight', 'encoder.block.20.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.22.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.20.layer.1.DenseReluDense.wi.weight', 'encoder.block.13.layer.0.SelfAttention.k.weight', 'encoder.block.15.layer.1.layer_norm.weight', 'encoder.block.17.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.18.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.16.layer.1.DenseReluDense.wo.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "embeddings = HuggingFaceInstructEmbeddings(model_name= \"hkunlp/instructor-large\",  model_kwargs={\"device\": \"cpu\"},query_instruction=\"Represent the query for retrieval: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e8c1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "e=embeddings.embed_query(\"What is your refund policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccd74800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04671666771173477,\n",
       " -0.004087879788130522,\n",
       " -0.015315327793359756,\n",
       " 0.029575854539871216,\n",
       " 0.05121871083974838]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50f977a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a32b6",
   "metadata": {},
   "source": [
    "As you can see above, embedding for a sentance \"What is your refund policy\" is a list of size 768. Looking at the numbers in this list, doesn't give any intuitive understanding of what it is but just assume that these numbers are capturing the meaning of \"What is your refund policy\". If you are curious to know about embeddings, go to youtube and search \"codebasics word embeddings\" and you will find bunch of videos with simple, intuitive explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f375f01",
   "metadata": {},
   "source": [
    "### Vector store using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2bcd297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\rohan\\anaconda3\\lib\\site-packages (1.7.4)\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f79a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb=FAISS.from_documents(embedding=embeddings,documents=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fd260a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='prompt: Once purchased, is this course available for lifetime access?\\nresponse: Yes', metadata={'source': 'Once purchased, is this course available for lifetime access?', 'row': 22}),\n",
       " Document(page_content='prompt: why row and value option is not showing for the visual in PowerBI , any setting need to be change, please let me know?\\nresponse: You have selected Table Visual instead of Matrix. That is why you are seeing a different interface.', metadata={'source': 'why row and value option is not showing for the visual in PowerBI , any setting need to be change, please let me know?', 'row': 46}),\n",
       " Document(page_content='prompt: Can I add this course to my resume? If Yes, how?\\nresponse: Absolutely, we have a section in this course explaining how you can add the learnings from this course in your resume that will appeal to the hiring team.', metadata={'source': 'Can I add this course to my resume? If Yes, how?', 'row': 34}),\n",
       " Document(page_content='prompt: I use tableau, can I take this course?\\nresponse: Yes, you will still benefit from the concepts outside the tools that are discussed in this course such as business context, problem solving, project management tools, etc.', metadata={'source': 'I use tableau, can I take this course?', 'row': 28})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retreiver=vectordb.as_retriever()\n",
    "rdocs=retreiver.get_relevant_documents(\"for how long is this course valid?\")\n",
    "rdocs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b9d7c",
   "metadata": {},
   "source": [
    "### Create RetrievalQA chain along with prompt template ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70f6a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template=\"\"\"Given the following context and a question, generate an answer based on this contest only.If the answer\n",
    "is not found in the context, kindly state \"I don't know.\" Don't try to makeup an answer.\n",
    "\n",
    "CONTEXT: {context}\n",
    "QUESTION: {question}\"\"\"\n",
    "\n",
    "PROMPT=PromptTemplate(template=prompt_template,input_variables=['context','question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83e20755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "chain=RetrievalQA.from_chain_type(\n",
    "    chain_type='stuff',\n",
    "    retriever=retreiver,\n",
    "    llm=llm,\n",
    "    input_key='query',\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={'prompt':PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c901a7e",
   "metadata": {},
   "source": [
    "### We are all set ðŸ‘ðŸ¼ Let's ask some questions now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6c6b632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Do you guys provide internships and offer EMI payments?',\n",
       " 'result': 'Yes, we provide internships and offer EMI payments.',\n",
       " 'source_documents': [Document(page_content='prompt: Do you provide any virtual internship?\\nresponse: Yes', metadata={'source': 'Do you provide any virtual internship?', 'row': 14}),\n",
       "  Document(page_content=\"prompt: whenever I am loading fact_sales_monthly table in Power query not all columns are visible\\nresponse: Have you taken SQL course in Codebasics first ?\\nIf yes, delete the existing databases in MySQL workbench and re-import again. After this go to 'Home' tab in Power Query and click on Refresh button and see if it works any.\\n\\nCheck this reference too: https://discordapp.com/channels/1090613684163850280/1111180401478746163/1111190746507251732\", metadata={'source': 'whenever I am loading fact_sales_monthly table in Power query not all columns are visible', 'row': 60}),\n",
       "  Document(page_content='prompt: What if I donâ€™t like this bootcamp?\\nresponse: As promised we will give you a 100% refund based on the guidelines (Please refer to our course refund policy before enrolling).', metadata={'source': 'What if I donâ€™t like this bootcamp?', 'row': 6}),\n",
       "  Document(page_content=\"prompt: The fact_sales_monthly table seems to be missing. Could you please provide information about why it is not available?\\nresponse: Delete the existing databases in MySQL workbench and re-import again. After this go to 'Home' tab in Power Query and click on Refresh button and see if it works any.\", metadata={'source': 'The fact_sales_monthly table seems to be missing. Could you please provide information about why it is not available?', 'row': 61})]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"Do you guys provide internships and offer EMI payments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85403a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Do know capital of India?',\n",
       " 'result': \"I don't know\",\n",
       " 'source_documents': [Document(page_content='prompt: What is the duration of this bootcamp? How long will it last?\\nresponse: You can complete all courses in 3 months if you dedicate 2-3 hours per day.', metadata={'source': 'What is the duration of this bootcamp? How long will it last?', 'row': 8}),\n",
       "  Document(page_content='prompt: Why we use Net error in place of absolute net error\\nresponse: Directional Insight: The net error metric offers valuable information regarding the direction of the error in forecasting, distinguishing between positive (overestimation) and negative (underestimation) values. This enables the identification of specific products that consistently outperform or underperform relative to the expected values. Additionally, net error provides insights into the risk factor, indicating if there are instances of stockouts or excessive inventory levels.', metadata={'source': 'Why we use Net error in place of absolute net error', 'row': 71}),\n",
       "  Document(page_content='prompt: What datasets are used in this bootcamp? Is it some toy datasets or\\n something that mimics a real-world business problem?\\nresponse: The datasets used in this bootcamp are crafted from scratch to mimic the real business world by consolidating our years of experience. The datasets not just contain more than a million rows but also cover multiple facets of the business.', metadata={'source': 'What datasets are used in this bootcamp? Is it some toy datasets or\\n something that mimics a real-world business problem?', 'row': 3}),\n",
       "  Document(page_content='prompt: Iâ€™m not sure if this course is good enough for me to invest some money. What can I do?\\nresponse: Donâ€™t worry. Many videos in this course are free so watch them to get an idea of the quality of teaching. Dhaval Patel (the course instructor) runs a popular data science YouTube channel called Codebasics. On that, you can watch his videos and read comments to get an idea of his teaching style', metadata={'source': 'Iâ€™m not sure if this course is good enough for me to invest some money. What can I do?', 'row': 20})]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\"Do know capital of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75e5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
